---
title: "PS5"
author: "Kohan Chen, Katherine Tu"
date: "2024-11-07"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID): Kohan Chen, kohanchen
    - Partner 2 (name and cnet ID): Katherine Tu, 
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: **KC KT**
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[KC KT](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
import pandas as pd
import altair as alt
import time

import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
```


## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)

```{python}
import pandas as pd
import requests
from bs4 import BeautifulSoup

url = "https://oig.hhs.gov/fraud/enforcement/"

response = requests.get(url)
soup = BeautifulSoup(response.content, 'lxml')

titles = []
dates = []
categories = []
links = []

entries = soup.find_all('h2', class_='usa-card__heading')

for entry in entries:
    link = entry.find('a')
    if link:
        titles.append(link.text.strip())
        links.append('https://oig.hhs.gov' + link['href'])
    
    div = entry.find_parent('div')
    if div:
        date = div.find('span', class_='text-base-dark padding-right-105')
        dates.append(date.text.strip() if date else 'No date found')

        category = div.find('li', class_='display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1')
        categories.append(category.text.strip() if category else 'No category found')

print(f"Lengths - Titles: {len(titles)}, Dates: {len(dates)}, Categories: {len(categories)}, Links: {len(links)}")


df = pd.DataFrame({
    'Title': titles,
    'Date': dates,
    'Category': categories,
    'Link': links
})

df.head()
```

  
### 2. Crawling (PARTNER 1)

```{python}
import pandas as pd
import requests
from bs4 import BeautifulSoup

url = "https://oig.hhs.gov/fraud/enforcement/"

response = requests.get(url)
soup = BeautifulSoup(response.content, 'lxml')

titles = []
dates = []
categories = []
links = []
agencies = [] 

for entry in entries:
    link = entry.find('a')
    if link:
        titles.append(link.text.strip())
        full_link = 'https://oig.hhs.gov' + link['href']
        links.append(full_link)
       
        try: 
            response = requests.get(full_link)
            detailed_soup = BeautifulSoup(response.content, "html.parser")

            detail_div = detailed_soup.find('div', class_='margin-top-5 padding-y-3 border-y-1px border-base-lighter')
            if detail_div: 
                all_li = detail_div.find_all('li')
                agency_found = False
                for li in all_li:
                    span = li.find('span')
                    if span and 'Agency:' in span.text: 
                        agencies.append(li.text.replace('Agency:', '').strip())
                        agency_found = True
                        break 
                if not agency_found: 
                    agencies.append('No agency found')
        except Exception as e:
            print(f"Error processing {full_link}: {str(e)}")
            agencies.append('Error retrieving agency')
    
    div = entry.find_parent('div')
    if div:
        date = div.find('span', class_='text-base-dark padding-right-105')
        dates.append(date.text.strip() if date else 'No date found')

        category = div.find('li', class_='display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1')
        categories.append(category.text.strip() if category else 'No category found')

print(f"Lengths - Titles: {len(titles)}, Dates: {len(dates)}, Categories: {len(categories)}, Links: {len(links)}")


df = pd.DataFrame({
    'Title': titles,
    'Date': dates,
    'Category': categories,
    'Link': links,
    'Agency': agencies
})

df.head()

```

## Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2)
```{python}
def scrape_enforcement_actions(year, month):
    """
    Pseudo-code for scraping enforcement actions from a given month/year to present
    
    Parameters:
    year (int): Starting year to scrape from
    month (int): Starting month to scrape from
    """
```
FUNCTION scrape_enforcement_actions(year, month):

1. INPUT VALIDATION
   IF year is less than 2013
      Print "Please enter a year >= 2013"
      Exit function
   
2. SETUP
   Create empty lists for storing:
   - titles
   - dates
   - categories
   - links
   - agencies
   Set base URL to HHS OIG enforcement page

3. PAGINATION LOOP (WHILE loop)
   WHILE there are more pages to process:
      Get current page content
      FOR each enforcement action on page:
         Extract title and link
         Get agency from detailed page
         Get date and category
         Store all information in lists
         Wait 1 second before next action
      
      Look for "next page" link
      IF found:
         Update URL to next page
         Wait 1 second before next page
      ELSE:
         Exit WHILE loop

4. DATA PROCESSING
   Create DataFrame from collected lists
   Convert dates to proper format
   Filter to keep only entries >= input month/year

5. SAVE RESULTS
   Create filename: "enforcement_actions_[year]_[month].csv"
   Save DataFrame to CSV file

6. OUTPUT
   Return the filtered DataFrame

* b. Create Dynamic Scraper (PARTNER 2)

I will use ThreadPoolExecutor to get data much more faster than the regular process. And I used several strategies to avoid frequent timeout and connection errors and consistent failures on specific pages (like pages 192 and 193).

```{python}
import pandas as pd
import requests
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
import random
import time
import sys

# 增加递归限制
sys.setrecursionlimit(10000)

def fetch_detailed_info(link, session=None, max_retries=3):
    if session is None:
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
    
    for attempt in range(max_retries):
        try:
            response = session.get(link, timeout=10)
            if response.status_code == 200:
                detailed_soup = BeautifulSoup(response.content, "html.parser")
                agency = 'No agency found'
                detail_div = detailed_soup.find('div', class_='margin-top-5 padding-y-3 border-y-1px border-base-lighter')
                if detail_div:
                    all_li = detail_div.find_all('li')
                    for li in all_li:
                        span = li.find('span')
                        if span and 'Agency:' in span.text:
                            agency = li.text.replace('Agency:', '').strip()
                            break
                return agency
            
            if attempt < max_retries - 1:
                time.sleep(1)
                
        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep(1)
            else:
                print(f"Error processing {link}: {str(e)}")
    return 'Error retrieving agency'

def fetch_page_data(page_num, session=None, max_retries=3):
    if session is None:
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
    
    base_url = "https://oig.hhs.gov/fraud/enforcement/"
    for attempt in range(max_retries):
        try:
            current_url = f"{base_url}?page={page_num}" if page_num > 1 else base_url
            response = session.get(current_url, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                entries = soup.find_all('h2', class_='usa-card__heading')
                
                page_data = []
                for entry in entries:
                    link = entry.find('a')
                    if link:
                        title = link.text.strip()
                        full_link = 'https://oig.hhs.gov' + link['href']
                        div = entry.find_parent('div')
                        if div:
                            date = div.find('span', class_='text-base-dark padding-right-105')
                            date_text = date.text.strip() if date else 'No date found'
                            category_tags = div.find_all('li', class_='display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1')
                            entry_categories = [cat.text.strip() for cat in category_tags] if category_tags else ['NA']
                            page_data.append({
                                'Title': title,
                                'Date': date_text,
                                'Categories': entry_categories,
                                'Link': full_link
                            })
                return page_data if page_data else None
            
            if attempt < max_retries - 1:
                time.sleep(1)
                
        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep(1)
            else:
                print(f"Error on page {page_num}: {str(e)}")
    return None

def scrape_enforcement_actions(year, month, max_pages=482):
    if year < 2013:
        print("Please enter a year >= 2013")
        return None
    
    data_list = []
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    })
    
    # 第一阶段：快速并行获取页面
    print("Fetching pages...")
    failed_pages = set()
    
    with ThreadPoolExecutor(max_workers=8) as executor:
        future_to_page = {
            executor.submit(fetch_page_data, page, session): page 
            for page in range(1, max_pages + 1)
        }
        
        for future in as_completed(future_to_page):
            page = future_to_page[future]
            try:
                page_data = future.result()
                if page_data:
                    data_list.extend(page_data)
                    print(f"Added data from page {page}")
                else:
                    failed_pages.add(page)
            except Exception as e:
                failed_pages.add(page)
                print(f"Error on page {page}: {str(e)}")
    
    # 第二阶段：单独处理失败的页面
    if failed_pages:
        print(f"\nRetrying {len(failed_pages)} failed pages...")
        for page in sorted(failed_pages):
            try:
                page_data = fetch_page_data(page, session, max_retries=5)
                if page_data:
                    data_list.extend(page_data)
                    failed_pages.remove(page)
                    print(f"Successfully retrieved page {page} in retry")
            except Exception as e:
                print(f"Failed to retrieve page {page} in retry: {str(e)}")
    
    print(f"\nCollected {len(data_list)} total entries")
    
    # 第三阶段：快速获取机构信息
    print("Fetching agency information...")
    with ThreadPoolExecutor(max_workers=10) as executor:
        future_to_entry = {
            executor.submit(fetch_detailed_info, entry['Link'], session): i 
            for i, entry in enumerate(data_list)
        }
        
        completed = 0
        total = len(data_list)
        
        for future in as_completed(future_to_entry):
            try:
                idx = future_to_entry[future]
                agency = future.result()
                data_list[idx]['Agency'] = agency
                completed += 1
                if completed % 20 == 0:
                    print(f"Processed {completed}/{total} entries")
            except Exception as e:
                print(f"Error processing agency info: {str(e)}")

    # Create separate rows version
    rows_separate = []
    for entry in data_list:
        for category in entry['Categories']:
            rows_separate.append({
                'Title': entry['Title'],
                'Date': entry['Date'],
                'Category': category,
                'Link': entry['Link'],
                'Agency': entry.get('Agency', 'Error retrieving agency')
            })

    # Create combined categories version
    rows_combined = []
    for entry in data_list:
        rows_combined.append({
            'Title': entry['Title'],
            'Date': entry['Date'],
            'Category': ' and '.join(entry['Categories']),
            'Link': entry['Link'],
            'Agency': entry.get('Agency', 'Error retrieving agency')
        })

    # Create DataFrames
    df_separate = pd.DataFrame(rows_separate)
    df_combined = pd.DataFrame(rows_combined)
    
    # Convert date and filter
    for df in [df_separate, df_combined]:
        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
        df = df[df['Date'] >= f"{year}-{month:02d}-01"]
    
    # Save to CSV
    filename_separate = f"enforcement_actions_{year}_{month}_separate.csv"
    filename_combined = f"enforcement_actions_{year}_{month}_combined.csv"
    
    df_separate.to_csv(filename_separate, index=False)
    df_combined.to_csv(filename_combined, index=False)
    
    print(f"\nSaved separate categories version to: {filename_separate}")
    print(f"Saved combined categories version to: {filename_combined}")
    
    return df_combined

if __name__ == "__main__":
    try:
        df = scrape_enforcement_actions(2023, 1)
        if df is not None:
            print(f"\nTotal number of enforcement actions: {len(df)}")
            print("\nEarliest enforcement action:")
            earliest = df.sort_values('Date').iloc[0]
            print(f"Date: {earliest['Date']}")
            print(f"Title: {earliest['Title']}")
            print(f"Agency: {earliest['Agency']}")
            print(f"Categories: {earliest['Category']}")
    except Exception as e:
        print(f"Error running scraper: {str(e)}")
```

Total number of enforcement actions: 9623
Earliest enforcement action:
Date: 2012-06-20 00:00:00
Title: One Individual Charged with Failure to Pay Legal Child Support
Agency: U.S. Attorney's Office District of Puerto Rico
Categories: Child Support and Criminal and Civil Actions


* c. Test Partner's Code (PARTNER 1)

```{python}
# Run scraper for January 2021
df = pd.read_csv('/Users/kohanchen/Documents/Fall 2024/student30538/problem_sets/ps5/PS5/enforcement_actions_2023_1_combined.csv')
df['Date'] = pd.to_datetime(df['Date'])

df_filtered = df[df['Date'] >= '2021-01-01']

#Print results
print(f"\nTotal number of enforcement actions since Jan 2021: {len(df)}")

# Get earliest action details
print(f"\nTotal number of enforcement actions since Jan 2021: {len(df_filtered)}")
print("\nEarliest enforcement action:")
earliest = df_filtered.sort_values('Date').iloc[0]
print(f"Date: {earliest['Date']}")
print(f"Title: {earliest['Title']}")
print(f"Agency: {earliest['Agency']}")
print(f"Categories: {earliest['Category']}")

```

## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)

```{python}

```

### 2. Plot the number of enforcement actions categorized: (PARTNER 1)

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}

```

* based on five topics

```{python}

```

## Step 4: Create maps of enforcement activity

### 1. Map by State (PARTNER 1)

```{python}

```


### 2. Map by District (PARTNER 2)

```{python}

```

## Extra Credit

### 1. Merge zip code shapefile with population
```{python}

```

### 2. Conduct spatial join
```{python}

```

### 3. Map the action ratio in each district
```{python}

```