---
title: "PS5"
author: "Kohan Chen, Katherine Tu"
date: "2024-11-07"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID): Kohan Chen, kohanchen
    - Partner 2 (name and cnet ID): Katherine Tu, 
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: **KC KT**
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[KC KT](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
import pandas as pd
import altair as alt
import time

import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
```


## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)

```{python}
import pandas as pd
import requests
from bs4 import BeautifulSoup

url = "https://oig.hhs.gov/fraud/enforcement/"

response = requests.get(url)
soup = BeautifulSoup(response.content, 'lxml')

titles = []
dates = []
categories = []
links = []

entries = soup.find_all('h2', class_='usa-card__heading')

for entry in entries:
    link = entry.find('a')
    if link:
        titles.append(link.text.strip())
        links.append('https://oig.hhs.gov' + link['href'])
    
    div = entry.find_parent('div')
    if div:
        date = div.find('span', class_='text-base-dark padding-right-105')
        dates.append(date.text.strip() if date else 'No date found')

        category = div.find('li', class_='display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1')
        categories.append(category.text.strip() if category else 'No category found')

print(f"Lengths - Titles: {len(titles)}, Dates: {len(dates)}, Categories: {len(categories)}, Links: {len(links)}")


df = pd.DataFrame({
    'Title': titles,
    'Date': dates,
    'Category': categories,
    'Link': links
})

df.head()
```

  
### 2. Crawling (PARTNER 1)

```{python}
import pandas as pd
import requests
from bs4 import BeautifulSoup

url = "https://oig.hhs.gov/fraud/enforcement/"

response = requests.get(url)
soup = BeautifulSoup(response.content, 'lxml')

titles = []
dates = []
categories = []
links = []
agencies = [] 

for entry in entries:
    link = entry.find('a')
    if link:
        titles.append(link.text.strip())
        full_link = 'https://oig.hhs.gov' + link['href']
        links.append(full_link)
       
        try: 
            response = requests.get(full_link)
            detailed_soup = BeautifulSoup(response.content, "html.parser")

            detail_div = detailed_soup.find('div', class_='margin-top-5 padding-y-3 border-y-1px border-base-lighter')
            if detail_div: 
                all_li = detail_div.find_all('li')
                agency_found = False
                for li in all_li:
                    span = li.find('span')
                    if span and 'Agency:' in span.text: 
                        agencies.append(li.text.replace('Agency:', '').strip())
                        agency_found = True
                        break 
                if not agency_found: 
                    agencies.append('No agency found')
        except Exception as e:
            print(f"Error processing {full_link}: {str(e)}")
            agencies.append('Error retrieving agency')
    
    div = entry.find_parent('div')
    if div:
        date = div.find('span', class_='text-base-dark padding-right-105')
        dates.append(date.text.strip() if date else 'No date found')

        category = div.find('li', class_='display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1')
        categories.append(category.text.strip() if category else 'No category found')

print(f"Lengths - Titles: {len(titles)}, Dates: {len(dates)}, Categories: {len(categories)}, Links: {len(links)}")


df = pd.DataFrame({
    'Title': titles,
    'Date': dates,
    'Category': categories,
    'Link': links,
    'Agency': agencies
})

df.head()

```

## Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2)
```{python}
def scrape_enforcement_actions(year, month):
    """
    Pseudo-code for scraping enforcement actions from a given month/year to present
    
    Parameters:
    year (int): Starting year to scrape from
    month (int): Starting month to scrape from
    """
```
FUNCTION scrape_enforcement_actions(year, month):

1. INPUT VALIDATION
   IF year is less than 2013
      Print "Please enter a year >= 2013"
      Exit function
   
2. SETUP
   Create empty lists for storing:
   - titles
   - dates
   - categories
   - links
   - agencies
   Set base URL to HHS OIG enforcement page

3. PAGINATION LOOP (WHILE loop)
   WHILE there are more pages to process:
      Get current page content
      FOR each enforcement action on page:
         Extract title and link
         Get agency from detailed page
         Get date and category
         Store all information in lists
         Wait 1 second before next action
      
      Look for "next page" link
      IF found:
         Update URL to next page
         Wait 1 second before next page
      ELSE:
         Exit WHILE loop

4. DATA PROCESSING
   Create DataFrame from collected lists
   Convert dates to proper format
   Filter to keep only entries >= input month/year

5. SAVE RESULTS
   Create filename: "enforcement_actions_[year]_[month].csv"
   Save DataFrame to CSV file

6. OUTPUT
   Return the filtered DataFrame
* b. Create Dynamic Scraper (PARTNER 2)
I will use ThreadPoolExecutor to get data much more faster than the regular process. And I used several strategies to avoid time out problem.

```{python}
import pandas as pd
import requests
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
import random
import time
import sys

# 增加递归限制
sys.setrecursionlimit(10000)

def fetch_page(url):
    try:
        response = requests.get(url, timeout=10)
        return BeautifulSoup(response.content, 'html.parser')  # 改用 html.parser 而不是 lxml
    except Exception as e:
        print(f"Error fetching page {url}: {str(e)}")
        return None

def fetch_detailed_info(link):
    try:
        response = requests.get(link, timeout=10)
        detailed_soup = BeautifulSoup(response.content, "html.parser")
        
        agency = 'No agency found'
        detail_div = detailed_soup.find('div', class_='margin-top-5 padding-y-3 border-y-1px border-base-lighter')
        if detail_div:
            all_li = detail_div.find_all('li')
            for li in all_li:
                span = li.find('span')
                if span and 'Agency:' in span.text:
                    agency = li.text.replace('Agency:', '').strip()
                    break
        return agency
    except Exception as e:
        print(f"Error processing {link}: {str(e)}")
        return 'Error retrieving agency'

def scrape_enforcement_actions(year, month, max_pages=482):
    if year < 2013:
        print("Please enter a year >= 2013")
        return None
    
    data_list = []
    base_url = "https://oig.hhs.gov/fraud/enforcement/"
    page = 1

    while page <= max_pages:
        try:
            print(f"Processing page {page}...")
            current_url = f"{base_url}?page={page}" if page > 1 else base_url
            
            soup = fetch_page(current_url)
            if soup is None:
                print(f"Failed to fetch page {page}")
                break
                
            entries = soup.find_all('h2', class_='usa-card__heading')
            
            if not entries:
                print(f"No entries found on page {page}. Stopping pagination.")
                break
            
            entries_added = 0
            for entry in entries:
                link = entry.find('a')
                if link:
                    title = link.text.strip()
                    full_link = 'https://oig.hhs.gov' + link['href']
                    
                    div = entry.find_parent('div')
                    if div:
                        date = div.find('span', class_='text-base-dark padding-right-105')
                        date_text = date.text.strip() if date else 'No date found'
                        
                        category_tags = div.find_all('li', class_='display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1')
                        entry_categories = [cat.text.strip() for cat in category_tags] if category_tags else ['NA']
                        
                        data_list.append({
                            'Title': title,
                            'Date': date_text,
                            'Categories': entry_categories,
                            'Link': full_link
                        })
                        entries_added += 1
            
            if entries_added == 0:
                print(f"No valid entries added on page {page}. Stopping pagination.")
                break
            
            print(f"Added {entries_added} entries from page {page}")
            page += 1
            time.sleep(random.uniform(1, 2))  # 增加延迟时间
            
        except Exception as e:
            print(f"Error processing page {page}: {str(e)}")
            break

    print(f"\nCollected {len(data_list)} total entries")
    
    # Fetch agency info with reduced parallelism
    with ThreadPoolExecutor(max_workers=5) as executor:  # 减少并行数
        futures = {executor.submit(fetch_detailed_info, entry['Link']): i 
                  for i, entry in enumerate(data_list)}
        
        for future in as_completed(futures):
            try:
                idx = futures[future]
                agency = future.result()
                data_list[idx]['Agency'] = agency
            except Exception as e:
                print(f"Error processing agency info: {str(e)}")

    # Create separate rows version
    rows_separate = []
    for entry in data_list:
        for category in entry['Categories']:
            rows_separate.append({
                'Title': entry['Title'],
                'Date': entry['Date'],
                'Category': category,
                'Link': entry['Link'],
                'Agency': entry.get('Agency', 'Error retrieving agency')
            })

    # Create combined categories version
    rows_combined = []
    for entry in data_list:
        rows_combined.append({
            'Title': entry['Title'],
            'Date': entry['Date'],
            'Category': ' and '.join(entry['Categories']),
            'Link': entry['Link'],
            'Agency': entry.get('Agency', 'Error retrieving agency')
        })

    # Create DataFrames
    df_separate = pd.DataFrame(rows_separate)
    df_combined = pd.DataFrame(rows_combined)
    
    # Convert dates and filter by date
    for df in [df_separate, df_combined]:
        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
        df = df[df['Date'] >= f"{year}-{month:02d}-01"]
    
    # Save to CSV
    filename_separate = f"enforcement_actions_{year}_{month}_separate.csv"
    filename_combined = f"enforcement_actions_{year}_{month}_combined.csv"
    
    df_separate.to_csv(filename_separate, index=False)
    df_combined.to_csv(filename_combined, index=False)
    
    print(f"\nSaved separate categories version to: {filename_separate}")
    print(f"Saved combined categories version to: {filename_combined}")
    
    return df_combined

# Run the scraper
if __name__ == "__main__":
    try:
        df = scrape_enforcement_actions(2023, 1)
        if df is not None:
            print(f"\nTotal number of enforcement actions: {len(df)}")
            print("\nEarliest enforcement action:")
            earliest = df.sort_values('Date').iloc[0]
            print(f"Date: {earliest['Date']}")
            print(f"Title: {earliest['Title']}")
            print(f"Agency: {earliest['Agency']}")
            print(f"Categories: {earliest['Category']}")
    except Exception as e:
        print(f"Error running scraper: {str(e)}")
```
* c. Test Partner's Code (PARTNER 1)

```{python}
# Run scraper for January 2021
df_2021_01 = scrape_enforcement_actions(2021, 1)

# Print total number of actions
print(f"\nTotal number of enforcement actions since Jan 2021: {len(df)}")

# Get earliest action details
earliest = df.sort_values('Date').iloc[0]
print("\nEarliest enforcement action details:")
print(f"Date: {earliest['Date'].strftime('%B %d, %Y')}")
print(f"Title: {earliest['Title']}")
print(f"Agency: {earliest['Agency']}")
print(f"Category: {earliest['Category']}")


# Display first few rows to verify data
print("\nFirst few entries:")
display(df_2021_01.head())

```
Total number of enforcement actions: 9623

Earliest enforcement action:
Date: 2012-06-20 00:00:00
Title: One Individual Charged with Failure to Pay Legal Child Support
Agency: U.S. Attorney's Office District of Puerto Rico
Categories: Child Support and Criminal and Civil Actions
## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)

```{python}

```

### 2. Plot the number of enforcement actions categorized: (PARTNER 1)

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}

```

* based on five topics

```{python}

```

## Step 4: Create maps of enforcement activity

### 1. Map by State (PARTNER 1)

```{python}

```


### 2. Map by District (PARTNER 2)

```{python}

```

## Extra Credit

### 1. Merge zip code shapefile with population
```{python}

```

### 2. Conduct spatial join
```{python}

```

### 3. Map the action ratio in each district
```{python}

```